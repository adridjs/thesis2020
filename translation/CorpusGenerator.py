import os
import random

from gender_bias.DataDriver import DataDriver


class CorpusGenerator:
    """
    Class used to generate the different corpus that are going to be analyzed.
    """
    def __init__(self, corpus_folder, save_dir=None, train_ratio=None, languages=None, genders=None, sampling_probs=None, prefixes=None):
        """

        :param train_ratio:
        :param languages:
        :param genders:
        :param sampling_probs:
        :param prefixes:
        """
        if sampling_probs and len(sampling_probs) != len(os.listdir(corpus_folder)):
            raise ValueError(f'Different length between sampling_probs {len(sampling_probs)} and number of files under {corpus_folder} ({len(os.listdir(corpus_folder))})')

        self.train_ratio = train_ratio or 0.8
        self.prefixes = prefixes or {'train': 'corpus.tc', 'test': 'test', 'dev': 'dev'}
        self.supported_corpus = ['mixed', 'europarl', 'biographies', 'balanced']
        self.datasets = {}
        self.dd = DataDriver(corpus_folder, save_dir=save_dir or corpus_folder, languages=languages, genders=genders)

    def generate_indices(self, size):
        """
        Generates the indices sampling from a given population. This is needed to get
        the aligned sentences in the forecoming languages, based on the initial indices
        that are generated by this function.
        :param size:
        :return:
        """
        indices = set(range(size))
        train_indices = set(random.sample(indices, int(self.train_ratio * len(indices))))
        remaining_indices = indices.difference(train_indices)
        test_indices = set(random.sample(remaining_indices, int(0.5 * len(remaining_indices))))
        dev_indices = remaining_indices.difference(test_indices)

        return train_indices, test_indices, dev_indices

    def split_dataset(self):
        """
        Splits the dataset by language into train, dev and test partitions.
        """

        sentence_pairs = tuple(self.datasets.items())
        dataset_length = len(sentence_pairs[0][1])
        train, test, dev = self.generate_indices(dataset_length)
        for language, sentences in sentence_pairs:
                with open(os.path.join(self.dd.save_dir, f'{self.prefixes["train"]}.{language}'), 'w+') as f:
                    x = [sentences[idx] for idx in train]
                    f.writelines(x)
                with open(os.path.join(self.dd.save_dir, f'{self.prefixes["test"]}.{language}'), 'w+') as f:
                    x = [sentences[idx] for idx in test]
                    f.writelines(x[:3000])
                with open(os.path.join(self.dd.save_dir, f'{self.prefixes["dev"]}.{language}'), 'w+') as f:
                    x = [sentences[idx] for idx in dev]
                    f.writelines(x[:3000])

    def _generate_biographies_corpus(self):
        """

        :param language:
        :return:
        """
        merged_genders = list()
        for language in self.dd.languages:
            for gender in self.dd.genders:
                sentences = list(map(str.strip,
                                     open(os.path.join(self.dd.corpus_folder,
                                                       f'{language}_{gender}.txt')).readlines()))
                sentences_without_name = list()
                for sentence in sentences:
                    if sentence:
                        clean_sent = self.dd.clean_sentence(sentence)
                        sentences_without_name.append(clean_sent)

                merged_genders.extend(sentences_without_name)

            with open(os.path.join(self.dd.save_dir, f'biographies.corpus.tc.{language}'), 'w+') as f:
                f.writelines("\n".join(merged_genders))

    def _generate_gender_balanced_corpus(self):
        """
        Generates a gender-balanced dataset between the :param languages and :param genders key sets.
        """
        # Get docs to quantify how many he/she instances exist in each language.
        docs, (least_docs_key, least_docs_value) = self.dd.get_biographies_corpus(format='txt')
        print(f'Key with least documents ({least_docs_value}): {least_docs_key}')

        # Balance dataset based on least_docs_val.
        for language in self.dd.languages:
            balanced_dataset = self.dd.get_balanced_corpus(docs, language, max_sentences=least_docs_value)
            with open(os.path.join(self.dd.save_dir, f'balanced.corpus.tc.{language}'), 'w+') as f:
                f.writelines("\n".join(balanced_dataset))

    def _generate_gebiocorpus_v2(self):
        for language in self.dd.languages:
            gebiocorpus = self.dd.get_gebiocorpus_v2(language)
            with open(os.path.join(self.dd.save_dir, f'gebio.corpus.tc.{language}'), 'w+') as f:
                f.writelines("\n".join(gebiocorpus))

    def _generate_mixed_corpus(self):
        """
        Generates a mixed corpus between EuroParl and biographies balanced by gender between the :param languages and :param genders key sets.
        """
        mixed_corpus = self.dd.load_corpus('mixed')
        for language in self.dd.languages:
            with open(os.path.join(self.dd.save_dir, f'mixed.corpus.tc.{language}'), 'w+') as f:
                f.writelines(mixed_corpus[language])

    def generate_corpus(self, corpus):
        if corpus != 'europarl' and corpus not in self.supported_corpus:
            raise ValueError(f'Supported corpus names are: {self.supported_corpus}')

        if corpus == 'biographies':
            self._generate_biographies_corpus()
        elif corpus == 'balanced':
            self._generate_gender_balanced_corpus()
        elif corpus == 'mixed':
            self._generate_mixed_corpus()
        elif corpus == 'gebio':
            self._generate_gebiocorpus_v2()

    def generate_partitions(self, corpus):
        if corpus not in self.supported_corpus:
            raise ValueError(f'Supported corpus names are: {self.supported_corpus}')

        self.datasets = self.dd.load_corpus(corpus)
        # split into train, test and dev
        self.split_dataset()

## Domain Adaptation
Update the weights of a pretrained transformer model with gender-balanced data, in order to obtain
a model with no gender bias in the domain space.

## MFT Dataset Generator
Creates a mixed dataset between 2 corpus (can be easily adapted to more) in order to apply the Mixed Fine Tuning
 strategy that has become a good way to perform domain adaption on pretrained models. This method cope with the
  "catastrophic forgetting" problem, giving to the network out-of-domain (general domain which the transformer
   has been trained on) samples together with in-domain samples
   with in-domain samples. 
   
   The MFTDatasetGenerator expects to have files in `data/` folder with the following pattern: 
   * For the general domain dataset, `corpus.clean.{language}` 
   * For the domain-specific dataset,  `{lang}_{gender}.txt` 
   
   The domain-specific dataset is the one generated by `word2vec.DataDriver.save_sentences`

### Examples
#### Generate a dataset with a proportion 3to1
```
# Generate dataset with 75% (3/4) and 25% (1/4) of sentences of sentences 
from in-domain and from out-of-domain datasets, respectively.
dg = MFTDatasetGenerator(corpus_folder, languages=languages)
dg.generate_mixed_dataset(repeat_in_domain=3)
```
